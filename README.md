# ETL PROJECT

## Objectives

- Download data from kaggle
- Create docker containers for airflow, DBT , POSTGRES
- Ingest data from local storage to PostgreSQL database
- Load the transformed data into Google BigQuery.
- Transform the data using dbt and prepare it for analytics.
- Analyze the data to answer specific business questions.

## Technology Stack

- Docker: To containerize and orchestrate the Airflow and dbt services.
- Airflow: For orchestrating the ETL processes.
- PostgreSQL: The source database for raw eCommerce data.
- Google BigQuery: The destination where transformed data is loaded and stored.
- dbt (Data Build Tool): To handle the transformations and modeling of the data.
- Python: Used in Airflow for automation scripts.
- GitHub: To manage code and version control.
- Loom (or similar):* For recording and summarizing the project.

---

## Steps to Run the Project 

### Prerequisites:
- Docker installed on your local machine.
- A Google Cloud Platform (GCP) project with BigQuery enabled.
- Service account credentials for BigQuery with access to the dataset.

### Steps:

1. Clone the repository:
   bash
   git clone https://github.com/your_username/ecommerce_pipeline.git
   cd ecommerce_pipeline
   

2. Set up environment variables:
   Create an .env file in the root directory and add the following variables:
   bash
   AIRFLOW_UID=50000
   AIRFLOW_IMAGE_NAME=apache/airflow:2.9.3
   

3. Configure Google Cloud Credentials:
   Ensure your BigQuery service account key is located at within you project folder
   
   
   

4. Build the Docker images:
   bash
   docker compose build
   

5. Start the services:
   bash
   docker compose up
   

6. Run the ETL pipeline:
   - Open Airflow at http://localhost:8080 and trigger the DAG that runs the ETL process (PostgreSQL to BigQuery).
   
7. Run dbt transformations:
   bash
   docker compose run dbt dbt run
   

8. *Inspect Results:*
   - Visit your BigQuery console and verify that the transformed data is loaded into the correct dataset.

---

##  dbt Models and Transformations

1. Raw Models:
   - raw_orders.sql: Extracts data from the PostgreSQL orders table and stores it in BigQuery.
   - raw_order_items.sql: Extracts data from the PostgreSQL order items table and stores it in BigQuery.
   
2. Staging Models:
   - stg_orders.sql: Cleans and standardizes the raw orders data. Adds calculated fields like order_total.
   - stg_order_items.sql: Cleans and standardizes the order items data, which includes calculating item totals.

3. Final Models:
   - fct_sales.sql: Builds the final sales fact table by joining stg_orders and stg_order_items.
   - dim_customers.sql: Builds the customer dimension table by transforming customer data and adding geographic details.
   
4. dbt Macros and Utils:
   - dbt_utils: A collection of utilities to simplify the development of complex SQL transformations.

---

## Analytical Questions Answered

- *What is the total revenue generated by the platform?*
  By aggregating the order_total field in the fct_sales table, we compute total revenue.
  
- *Which products are selling the most?*
  Using the stg_order_items model, we aggregate the quantity of products sold.

- *Who are the top customers?*
  By analyzing the dim_customers table, we rank customers based on their total spending.

---

##  Results and Insights

- The eCommerce platform generated significant revenue in the last quarter.
- The top-selling product categories are electronics and home appliances.
- Customer loyalty is high, with repeat purchases contributing to over 60% of revenue.

---



ecommerce_pipeline/
│
├── airflow/                     # Airflow DAGs and configurations
│   ├── dags/
│   └── config/
│
├── dbt/                         # dbt models and transformations
│   ├── models/
│   ├── macros/
│   └── profiles.yml
│
├── data/                        # Data-related files like the BigQuery key
│   └── key/
│
├── Dockerfile                   # Dockerfile for the dbt service
├── docker-compose.yml           # Docker Compose configuration
└── README.md                    # Instructions for setting up and running the project


---

